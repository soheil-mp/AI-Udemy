
# Importing the librarues       # the file with the .pth is the pre trained model which is the weights that we can uploat it to our model. 
import torch
from torch.autograd import Variable # This modulo is responsible for the gradietn descent
import cv2
from data import BaseTransform, VOC_CLASSES as labelmap # data is just a folder that contains the classes # BaseTransform is a class that will do the required transformations so that the input image will be compatiple with the neural network. because for inputing the images to the NN we need a certain format of the images # VOC_CLASSES is a dictionary that will do the ecoding of the classes. for example plane will be encoded as 1 and dog will be encoded as 2
from ssd import build_ssd # library of single shot multi-box model and then buid the ssd # the code is in the same repository
import imageio # library for processing the images of the video and applying the detect function on it

# Defining a function that will do the detections
def detect(frame, net, transform): # This function going to detect 3 arguments, first one is the image that detect function us going to applied and there is no need to take the black and white version of the image # second is the  SSD neural network # third argument is the transformation because there is gonig to be some transformation to the image. note: not the black and white transormation but the required format before going to the neural network. # output is going to be a rectangle with the name on that rectangle.
    height, width = frame.shape[:2] # first we want to get the height and width of the image or frame # shape is a attribute that returns a vector of 3 elements (height, width, # of channels) # number of channel for black and white is 1, for colored image is 3. # instead of writing [0,1] we write [:2]
    frame_t = transform(frame)[0] # Doing several transformation convert the image to a torch variable (that will be accepted into a SSD neural network) # steps: 1- Applying the 'transform' to have a right format (right dimention and right color). 2- conveert the numpy array to a torch tensor 3- Add a fake dimention (corresponce to the batch) to the torch tensor 4- convert to a torch variable (the class has been imported) which contains both the tensor and a gradient # this is the first step # frame_t is the transform frame # transform will return two elemts that we are only interested in the first one so we add abracket in here
    x = torch.from_numpy(frame_t).permute(2, 0, 1) # second transformation # torch is a advacnec matric that is useful in the neural network # from_numoy is a function that convets from the numpy array into torch tensors # permute is a sub-transformation that has not been mentioned and it's to convert the sequence of colors. in here we have RBG but in SSD neural net it's GRB. 'permute' do the same thing which we write permute(2,0,1) because green is index 2 and since the red is the first one, we put 0. since blue is the second index we put 1. so we want to from 0, 1, 2 to 2, 0, 1 
    x = Variable(x.unsqueeze(0)) # third step which first dimension corresponce to the batch and the other dimentions corrensponce to the input # we creating the fake dimention of the batch by the unsqeeze function # unsqeeze function takes one rgument which is the index of the dimension of the batch. remmber that the batch should always be the first dimention. then we put 0 inside unsqeeze function # we do the last two stp in the same line. # variable is the final step which we convet the x.unsqeeze(0) to the torch variable # we use x in here to because, we want to overwrite the previous x # now the input is ready to feed into the neural network
    y = net(x)    # output of the neural network SSD # feeding the x into the neural network # in here we use a trained ssd which is saved in ssd.py and its weight is saved under the ssd300_mAP_77.43_v2.pth # in here we put y instead of x because now we have the output. 
    detection = y.data # extracting important information that we need from the output # detection is a tensor that takes the data of y which corresponce to the tensor part of the torch variable. # torch variable consist of two elements(torch tensor and gradient) which by taking the first one we get the torch tensor. # detection = [batch, number of classes, number of occurance] # batch: because we created a fake dimension with the unsqeeze function (we have batch of input and output) # number of classes:  it means the number of objects that can ve detected such as dog, hourse, etc.  # number od occurance: this is number of occurance for each class. for example for the class number 2 which we have a dog, the number of occurance is 1. it means we only have one dog. # tuple of 5 elements: 1- score 2- x0 3- y0 4- x1 5- y1 this means that for each occurance of a class we get a score and coordinates of upper left corner and then the coordinate of lower right corner. in here we have a score for each occurance of a class such that if the score is below 0.6 then  occurance of the class won't be found in the image, if higher than 0.6 then it would be considered  
    scale = torch.Tensor([width, height, width, height]) # creating a new tensor object that have 4 dimentions of (WIDTH, HEIGHT, WIDTH, HEIGHT) # the reason for making such tensors is because the position of the detected object inside the image has to be normalized between 0 and 1. for doing so we need a scale tensor with these four dimension so to sum up, this new tensor (scale) will be just used to do this normalization between 0 and 1 of the positions of the object detected in the image # the first WIDTH,HEIGHT corresponce to the scale of the values in the upper left corner of th erectangle detector and second WIDTH,HEIGHT corresponse to the lower right corner of the same rectangle 
    for i in range(detection.size(1)): # Iterating through all classes and through all occurances of the classes # detection.size(1) is the number of classes
        j = 0 # j is the occurance of the class
        while detection[0, i, j, 0] >= 0.6: # while loop is exactly like 'if else' that has a loop # i, j is the occurance j of the class i which here we use index. first 0 is the batch. forth 0 is the (index of) score. so in genertal this is the score of the occurance j of the class i # the score should be higher than 0.6 in order for acceptence 
            pt = (detection[0, i, j, 1:] * scale).numpy() # keeping this occurance by a variable call pt which stants for point because we gonna keep that by the coordinates of upper left corner and lower right corner of the rectangle detector of occurance j of the class i # in detection firt we use batch, class i, occurance j but for the forth argument we don't use 0 because we are not interested in score any more and we want the coordinates which is 1: (it's the last four elements) # The reason for using scale because we want to apply the normalization # at the end we want to convert this torch tensor to numpy array because opencv works only on numpy array
            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) # drawing the rectangle # pt[0] is x0, pt[1] is y0, pt[2] is x1, pt[3] is y1 # just for safty we want to convert all these values to integers # Arguments in rectangle: 1- frame 2-3- the coordinate of the upper left cornder so we put the two numbers inside a paranthesis and we do this for the third argumnet which is the coordinate of the lower right corner 4- color of the rectangle in RGB 5-  Thickness 
            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA) # printing the label onto the rectangle by using the putText function # arguments in in putText: 1- frame 2- text to display which we put labelmap that is the shortcut name we gave to whatclasses. whatclasses is a dicionary that maps the name of the classes with numbers. here we put i(class) - 1 becuase the index starts at 0 so this is the index of i'th class 3- position of the text which we do in just above upper left corner of rectangle which is same as the previous line. 4- font of the text 5- size of the text 6- color of the text 7- thickness of the text 8- since we want the text be displayed continuesly (with continous lines and not little dots) so we use cv2.LINE_AA
            j += 1 # incrementing the occurance j 
    return frame 
            
# Creating the SSD neural network
net = build_ssd('test') # creting the neural network object # using build_ssd function from other code # takes only one argument which is the 'phase': test or train. since we have already trained it and have the traind file then we use test 
net.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage)) # loading the weights of an already pre-trained SSD neural network # this file was pre-trained to detect between 30 to 40 objects # since we're going to put them into a tensor then inside load_state_dict function, which is torch_load function. # torch_load function will open a tensor that will contain this weights # the use of load_state_function is to attribute these weights to our SSD neural network object # we need a second and third elements as follows

# Creating the transformation 
transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0)) # the BaseTransform class will do all the required transformation # first argument is net.size which is the target size of the images to feed to the neural network # second arguments is the tuple of 3 arguments which allow to put the col values at the right scale which the scale under which the neural network (pth file) was trained. don't worry the numbers we just put them to get the right scale. 

# Doing some Object Detection on a video     
reader = imageio.get_reader('funny_dog.mp4') # we use the imafeio library, there is also another library PIL which in here the fist one is much more better # the 2 second video has around 68 frames
fps = reader.get_meta_data()['fps'] # getting the frequence of the frames that is FPL (frame per second) frequence  
writer = imageio.get_writer('output.mp4' ,fps = fps) # create an output video with the same FPS sequence # in here we are not opening a video, we are creating one. so we use get_writer to create something like an object that contains a video and in this something we append and add the sequence of frames. the frames that we apply to detect function # there is 2 arguments: 1- name of the output video 2-  the frequence: how many frames per second do we want. the name of argument is fps
for i, frame in enumerate(reader): # looping every frame in the video to apply the detect function to print the rectangle # i is the number of image that is processed which in here is arounf 0 to 68. # ofcourse, we also iterate over the frames of the video
    frame = detect(frame, net.eval(), transform) # applying detect function # net is an advanced structure. it's an object of the build_ssd class. the reason for using .eval() is because to align with the way of build_ssd function. # net.eval() represents our neural network from which we get the output y (y=net()) and therefor detection on each frame # this frame is our original frame that we are overwiring it to be sure 
    writer.append_data(frame) # after getting each frame, we want to append it to 'writing'
    print(i) # just to see at which frame we reached
writer.close() # this will close the process and we get the output video in the same repository.

