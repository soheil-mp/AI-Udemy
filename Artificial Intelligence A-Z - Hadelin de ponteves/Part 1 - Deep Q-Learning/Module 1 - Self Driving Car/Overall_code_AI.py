# In the map.py we used AI in 3 part: 1- Importing the file 2- Creating an object of that file which is called brain and and some initialization that is until end of 3 lines after that  3- line 131 which is action = brian.update(last_reward, last_signal # Check out the outout latex file for the algorithms

# Importing the libraries
import numpy as np
import random # Just to taking some random samples from diffrent batches
import os # useful for the loading the model
import torch
import torch.nn as nn
import torch.nn.functional as F # This packages contians the diffrent functions that is going to be used while making the neural network
import torch.optim as optim
import torch.autograd as autograd
from torch.autograd import Variable


# Creating the architect of the Neural Network

class Network(nn.Module):
    def __init__(self, input_size, nb_action): # Our input size is 5 becuase out inuput vectors have 5 dimensions: 3 signals, orientation, minus orientation that are the vector of the encoded values that describe one state of the enviroment # singnals: 1- in front of us 2- left side 3- right side  and of course these are only for out application in reallity the signals are 360 signals (like the google) # orientations are to keep track of the goal we are about to reach # the output corresponce to the action which are going straight, right, or left.
        super(Network, self).__init__()
        self.inut_size = input_size
        self.nb_action = nb_action
        self.fc1 = nn.Linear(input_size, 30) # Full connections: 1- full connection between the input layer and hidden layer 2- full connection between the hidden layer and the output layer # Linear in here means that all of the neurons in input layer are connected to all of the neurons in hidden layer # Arguments: 1- Number of Neurons in the first layer 2- Number of Neurons in the second layer # The reason for choosing 30 is only by exprimenting
        self.fc2 = nn.Linear(30, nb_action)
        
    def forward(self, state): # this will activate the neurons and also it will return the Q values for each possible action depending on the input state # arguments: 1- self 2- input which in here is the input 
        x = F.relu(self.fc1(state)) # activating the hidden nerons by taking the input neurons and also using the firs full connection to get it and afterward applying the rectifier activation function on them # x represent the hidden neuron # arguments: inputs which are the neurons we want to activate. that is the hidden neurons.
        q_values = self.fc2(x) # Returning the output neurons. # this is not the output directly, this is out Q-Values
        return q_values


# Implementing exprience replay  
    
class ReplayMemory(object): # Since all of the AI is based on the Markov Descision Process in which it consists of looking at a series of events. for example events is like going from one state (S_t) to another state (S_t+1) which in this case since the next state is correlated with the current state then network will learn very well. In here instead of considering the current state (this is one state at time t) we are going to consider more in the past. so in here are events are not going to be S_t and S_t+1, this will be 100 states in the past (for example S_t-100 until S_t-1 and S_t) in other word in here we put 100 last transaction into the memory. # Onece this memory of the 100 transactions have benn implement, the next update is going to be based on the sample batches of the these memories 
    
    def __init__(self, capacity):
        self.capacity = capacity # Maximum number of transactions we want to have in our memory of events
        self.memory = [] # Memory contains the last 100 events
        
        
    def push(self, event): # push function does two tasks: 1- Appending the new transaction or event in the memory 2- Making sure thet memory has always 100 transactions (In here for simplicity we say 100, it might even be 100'000).  # the second argument is an 'event' since this function will append the events into the memory # event and transition are the same # event contains: 1- last state S_t  2- new state S_t+1  3- last action A_t  4- last reward
        self.memory.append(event) # Task one
        if len(self.memory) > self.capacity: # Task two using If condition
            del self.memory[0] # If the conndition is true deleting the first element of memory
            
            
    def sample(self, batch_size): # Getting random samples from the memory # second argument is the batch_size since we are taking samples of fixed size
        samples = zip(*random.sample(self.memory, batch_size)) # zip * has a similar charachteristic as reshape. for example if list = ((1,2,3),(4,5,6)), then zip(*list) = ((1,4), (2,3), (5,6)) so zip just reshapes the list. # The reason for using zip: since we are adding events to the memory, and event has the form (State, Action, Reward) which we don't want them (like this ((1,2,3),(4,5,6))) for our algoirhms. we want the format like the following. 3 samples in which one sample for State, another ones for Action and Reward like ((1,4), (2,3), (5,6)). to sum up: create one batch for each state, action and reward which afterward these are going to put them sepprately into the torch variable and each one will get a gradient # random is the library we have imported and from this library, we are using the sample function.  # arguments for sample function: 1- self: refrers to the self.memory that we have implemented 2- size of the batch we want to take from this memory
        return map(lambda x: Variable(torch.cat(x, 0)), samples) # Samples can return directly since we want them in the format of torch variable for doing so we use map which it will do the mapping from the samples to the torch variables that contains tensors and gradient # argument of map: 1- function that will do the mapping which in here is the samples to torch variables. 2- This is what we want to apply this function onto which that is going to be the samples. # VAriable function will do this conversion. the reason for using concatanating (cat) is because for each batch that contains in the sample (for example the batch of the actions A1, A2, A3, and other actions) we have to concatantaing them with respect to the first dimenstion which correspoce to the state. this concatanation will make everything well aligned that is in each row, the state and the action and the reward all correspoce to the same time t # second argument of cat is the dimenstion with respect to which we want to do the concatanation which is 0.
   
    
# Implementing Deep Q Learning
        
class dqn(): # stand for deep Q-learning
    
    def __init__(self, input_size, nb_action, gamma): # arguments: 1- self  2-3- Creating an object of class Network. for doing so we have to put its input. (optional 4-) Creating an object of the ReplayMemory class which is capacity but since we are using it nly once when we created the memory and not any more then we don't need to specify the capacity argument. we could but we won't. 4- gamma parameter in the deep Q learning model which is delay coefficient
        self.gamma = gamma # Creating and initializing the variables
        self.reward_window = [] # Reward window is the sliding window of the mean of the last 100 words which will be used to evaluate the evolution of the AI performance. what we want to see is that the mean of the last 100 word increasing with time. # We initialize it with a empty list and we append to it with mean of the reeward
        self.model = Network(input_size, nb_action) # Creating the neural network. model is an object of the Network calss # it arguments should also be written
        self.memory = ReplayMemory(100000) # Creating the memory # The argument is the capacity which correspoce to the number of transitions (events) which is last state, last action, last reward.  
        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001) # Getting the optimizer which we have imported already. # Arguments: 1- parameters: all the parameters that can cutomize your Adam optimizer, .parameters is to get access to the parameters of our model 2- learning rate: this can be optain by exprimenting 
        self.last_state = torch.Tensor(input_size).unsqueeze(0)  #Initialising the torch tensor # Creating the events in the 3 lines: last State, Action, and Reward  # Last state is a vector of 5 dimension which are 3 signals of the 3 sensors (left, straight, right) and orientation and minus orientation # This is a vector but in PyTorch, it needs to be a torch and it also needs to have one more (fake) dimension that corresponse to the batch. because last state will be the input of the nueral network. in neual netweok in general, the input vectors cannot be a simple vector by itself, it has to be in a batch. the network can only accept the batch of observations. # argument: 1- size of the tensor which we use the input size. later the input size will be 5 # fake dimension should be the first dimension. we add the fake dimension with unsqueeze then we input the index of this fake dimension
        self.last_action = 0 # Actions are going to be either 0, 1, or 2. then using the action rotation vector, we will convert thiese indexes to the angles of rotations which are -20, 0, 20. check out he action2rotation in map.py. based on action2rotation = [0, 20, -20], if action is 0, we get 0. if actions is 1 we get 20 and if the action is 2, we get -20.  # In here we initialize it to the 0
        self.last_reward = 0 # Last reward is a float number between -1 and 1.


    def select_action(self, state): # This is a function that selects the right action at each time which is the move for the car (right, straigh, left) at each time to reach the goal and avoid the obstacles # arguments: 1- self 2- the action we select comes from the output of the neural network because its output is the q value for each possible action. therefor the action that will be the output of the neual network depends on the input state. input state is our second argument. we name it state because the input of the neural network are the input states that are encoded in the vecetor of 5 dimension
        probs = F.softmax(self.model(Variable(state, volatile = True))*7) # We start with softmax. softmax is about get the best action to play at each time but also exploring diffrent actions at the same time. this is consists of generating a distribution of probabilities for each of the Q-values, Q-State action. which in here we are going to have one value for each action of going left, straight, right. we have one input state which is state in here and three possible action or three Q-value which are Q-state action 1, Q-state action 2, Q-state action 3. we are going to generate a distribuation of probabilities with repect to these three keys values and haveing one probabilities for each of the three Q-values. all of these probabilities are going to sum up to 1. we are going to do all of these with softmax and soft max will attribute a large probability to the highest Q value. # Another alternative for the softmax is simple R Max which is directly taking the maximum of Q values but in that case we are not exploring the other actions. that is why softmax is better to use. # probs refer to these probabilities # we are getting the softmax from the F library that we have imported # arguments: 1- the entities for which we want to generate a probability distribution. of course these entities are the Q-values. Q values are the output of the neural network. for taking them since we have already initialzed them then we use self.model # We need to get the output of the neural network model. therefor we are going to add some paranthesis which we are going to input the input state named state # state in here should be a torch tensor for later. because later we are going to use self.last_state to put it as a argument of select_action function. the state argument is going to become self.last.action later. # so state is a torch tensor and most of the tensor are wrapped into a variable that also contains the gradient. in here, since this is a input state then there is not going to be any diffrentiation so we don't need the gradient. so in here, we convert the torch tensor into a torch variable. to specify that we don't want the gradient then we add Volatile = True # this action will save us some memory and also some imporvement to our model.  # tempreture parameter is a parameter allows us to modulated how the neural network will be sure which action it should decide to play. tempreture parameter is a positive number, which closer to 0 the less sure the neural networl will be when playing in action, and the higer this parameter is, the more sure the neural network will be of the action it decides to play. for adding this parameter, multiply the output which are the Q-values by this tempreture parameter. so let's start for example with 7 (T=7). the smaller the tempreture parameter is, the more car will behave liker the insect, by increasing this parameter, it will behave more like the car # remmber that this part is really important because if we assign it to 0 then we are NOT going to have a AI in game and by increasing the number we get a stronger AI. in here the best number is 100 which after exprimenting change 7 to that. # for deactivating AI put 0. remmber also don't increase this number so much since the AI is going to have less exploring # softmax([1,2,3]) = [0.04, 0.11, 0.85] =>  softmax([1,2,3] * 3) = [0, 0.02, 0.98] by having a tempreture parameter the car is more sure about the action it should take.                                                                                                                                                                  
        action = probs.multinomial() # the priciple of the softmax method is not only to generate the probability distribution for each of the Q-values but also.. the second step # Taking the random draw of the probability distribution that we have just created to get the final action. we doing this by .multinomial()
        return action.data[0,0] # Since the probs that multinil returns pytorch variable with fake batch (fake dimension corresponding to this batch). to get the right result that we want that is the action 0, action 1, action 2. then we need to add .data[0,0] which the result is in this index.
         

    def learn(self, batch_state, batch_next_state, batch_reward, batch_action): # Training the deep neural network for doing the process of forward-propagation and backward-propagation, and after comparing the the output with target to compute the loss error. then back propage the loss error into a new network and using stochastic gradient descent to update the weights according to how much they are contributed to the loss error. # arguments: 1- self  2- current state which is batch_state  3- batch of next state which is batch_next_state 4- batch_reward 5- batch-action # these are transitions of markov decision process # the reason for taking these in batches is becuase we have created sample batches in simple function so our transitions are in the form of the batches
        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).sqeeze(1)  # Getting the output of the batch_state # The reason for using the self.model is because we want to get the model outputs of the input states of the batch state and since out model is expecting the batch of input state then we input the batch_state directly. # self.model(batch_state) will get the output of all of the possible actions which are 0, 1, 2 which we are not interested in all of them. we are only interested in the actions that have been chosen (the action that decided by the network to play). we choose the actions that have been played by .gather(1, batch_action)  # Note that batch_state has a fake dimension that batch_action doesn't have. we fix this by unsqueze(1). the reason for choosing 1 instead 0 is because 0 correspince to the fake dimesion of the state and 1 corrensponce to the fake dimension of the actions # At the end we want to kill this fake batch with squeeze because we are out of the network now and we don't want the outputs in batch anymore, we want them in a simple tensor vector
        next_outputs = self.model(batch_next_state).detach().max(1)[0]  # This is th next output which is the maximum of the Q-values for the next state with respect to all the actions # This is the result of out neural network when the input is the batch_next_state. # Also check out the algorithms in LaTex # detach is used to detach all of the outputs of the model becuase we have several states in the batch_next_state that is the batch of all states in all the transitions taken from the random sample of our memory # after detaching them we want to take the maximmum of all of these Q-values and since we take these maximums with respect to the action then we have to specify that. since the action is represended by index one then we put 1 iniside the maximum. # [0]: because we have to specify that we take the Q-values of S-t+1 that is the next state and it is represented with the index 0 # so we get the maximum of the Q-values of the next state represented by the index 0 according to all of the actions that represented by the index 1.
        target = self.gamma*next_outputs + batch_reward # check out the formula in the pdf. 
        td_loss = F.smooth_l1_loss(outputs, target)  # computing the loss # td stands for temporal diffrence # this is stand for huber loss that imporoves a lot the Q learning that is equal to the smooth_l1_loss # arguments: 1- prediction which in here it's 'outputs' 2- target
        self.optimizer.zero_grad() # back propagating the error back into the network to update the weights with stochastic gradient descent but before that.. # we have initialized the optimizer before. remmber that when working with pytorch we need to re-initialize again at each iteration of the loop so in here we have to reinitialize the optimizer from one iteration to the other in the loop of stochastic gradient descent and for reinitialize at each iteration of the loop we are going to use zero_grad
        td_loss.backward(retain_variable = True) # backward propagation with out optimizer # (retain_variable = True) will improve backpropagation and it frees some memory
        self.optimizer.step() # updating the weights 


    def update(self, reward, new_signal): # this will update everything as soon as the AI reaches to the new state. when the AI reches to the new state, we need to update the action. the last action becomes the new action that was just played. the last state becomes the new state. and the last reward becomes the new reward when we play the action. so after selecting an action, we need to update all of the elements of the transitions. and afterward appending this new transition to the memory and finally update the reward window to keep an eye on the evolution of how training going and how the exploration going. # In here we make a connection to the map.py which inside the map.py we can see it inside the update function and with name of 'action' or """ action = brain.update(last_reward, last_signal) """ which action takes the last_reward and the last_signal to decide the next action to play. so in here we integrade the select_action funtion from above inside the update function in here so we can select the right action beside making the updates. # in here we make the .update(last_reward, last_signal) as our function and its parameters in here so we can use it in the game. note that since the name of parameter has been used above and to eliminating the confusion we change the name to update(reward, new_signal) # the last_reward will be defined in the game which if the car reaches to the sand and etc. # the reason for using self is because we are using self.memory in below so we have to specifu that in here.
        new_state = torch.Tensor(new_signal).float().unsqueeze(0) # updating the new state # getting the new state depends on the signals that sensor has been detected. so state is the signal itself. which signal in map.py consists of  last_signal=[self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation] and since this is going to the input of of the neural network then first we have to convert it to the torch tensor. # to make sure all of the elements are float then we add.float() # afterward we have to create a fake dimension that correspond to the batch which we do it with unsqeeze and then inpu the index of this fake dimension which we want to have for the batch which is 0 
        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward]))) # updating the memory. the reason for updating this is because at each time t, a transition composed of current state (S_t), next state (S_t+1), reward (RT), action (AT). already we got the S_t, RT, AT and we just got the last element of transition (S_t+1). by getting (S_t+1), we are getting one brand new transition of the memory and therefor we have to append this new transition to the memory because that's our next transition. # for updating this we have already made a function (push function) that does this. # inside push we add: 1- the new transition that we have just got which is last state  2- the new state that we just reached  3- the action which is the last_action in the __init__ # remmber that all of the elements including here should be a torch tensor. the first two are tensors but in the case of the third one, since it's just simply a number (0, 1 or 2) then for converting it to the tensors we should use the long tensor. long is a type that contains integers # At the end to make sure that inside this long tensor in integer then we should add int() # at the end we also at the pair of brackets for making a list first an then going to the torch tensor class. and also we get a long tensor one element which will be the last action of 0, 1 or 2 itself #  4- the last argument is the last reward that we get. in here we initialized it to 0 and iside the map.py it wil get change (check out the line 139-159). for the forth argument we have to make another conveersion but since this argument is in float and not integer then we don't use long tensor and instead we use the torch.Tensor alone. 
        action = self.select_action(new_state) # Since we are done with one transition and got the observation of the new state then it's time for playing the action for doing so we use select action function from above wichi it's input is the new state.
        if len(self.memory.memory) > 100: # after playing the action, the AI will get a reward and the feedback of that reward. the AI will learn from these rewards (only if we got more than 100 elements in memory which we make if statement for this) # Note than in self.memory.memory, self.memory refres to the object of the replay memory class or "self.memory = ReplayMemory(100000)". and the replay memory class has a attribute which is memory or "self.memory = []" which is the second memory
            batch_state, batch_next_state, batch_reward, batch_action = self.memory.sample(100)  # in here we can learn now but before that we have to get the random sample of the 100 transitions and we can do it with the sample function from the above. # sample fuction returns the diffrent batches: state at time t, state at time t+1, action at time t, reward at time t. so now we create new variabes in here. # the input for sample is the number of the transitions we want our AI to learn from which is 100
            self.learn(batch_state, batch_next_state, batch_reward, batch_action) # updating these 4 argument from the last line.
        self.last_action = action # updating the last action which is the action from 4 line before.
        self.last_state = new_state # updating the last state which is the new state since it has been excecuted
        self.last_reward = reward # updating the reward which is the argument of the update function and also it's same as the last_reward (not self.last_reward) in the map.py in "action = brain.update(last_reward, last_signal)"
        self.reward_window.append(reward) # Updating the reward_window that we haved initialized it in the __init__
        if len(self.reward_window) > 1000: # Making the reward_windo fixed size # to make sure that the reward_window won't get more than 1000 elements that is the 1000 means of the last 100 words
            del self.reward_window[0] # deleting the first element of this reward_window in case of exceeding
        return action # since this update function not only updates the diffrent elemnts of transistion in the reward_window but also it returns the action that was played when reaching new state
    
    def score(self): # Computing the score on the sliding window of reward or in other word computing the mean of all the rewards in the reward window.
        return sum(self.reward_window) / (len(self.reward_window) +1.) # the sum of all the rewards in the reward_window that are between -1 and +1 divided by the total number of elements. # note that the denominator shouldn't be equal to 0 at any possibilities. for doing so we add a +1.
    
    def save(self):  # In here we are not going to save the whole model. we will only save our neural network which is self.model and we also save the self.optimizer because we only want to save the last weight at the last iteration and also the last version of the optimizer becuase it is connected to the weights
        torch.save({'state_dict': self.model.state_dict(), # inside paranthesis we have to put the dictionary
                    'optimizer': self.optimizer.state_dict,
                    }, 'last_brain.pth') # name of the file 
        
    def load(self):
        if os.path.isfile('last_brain.pth'): # Since out file is last_brain.pth, first we want to make sure that the file exists. is file will return true if the file exist
            print("=> loading checkpoint...")
            checkpoint = torch.load('last_brain.pth') 
            self.model.load_state_dict(checkpoint['state_dict']) # uploading the model and optimizer seperately and updating them with the current model and optimizer which first we do this by load state dict # checkpoint['state_dict']: we saved the model in name of state_dict in above
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            print("done !")
        else: 
            print("no checkpoint found...")
        
# This algorithm can have some imrovement: check out here for improvement https://www.udemy.com/artificial-intelligence-az/learn/v4/questions/2604982