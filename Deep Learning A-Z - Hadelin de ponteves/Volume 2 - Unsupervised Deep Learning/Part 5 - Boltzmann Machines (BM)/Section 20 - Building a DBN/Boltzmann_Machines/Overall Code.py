# this is a recommended system. it's in 2 parts: 1. does the user like the movie? (boltzman machine) 2. rating from 1 to 5 (auto encoder)

# Imporint the libraries
import numpy as np
import pandas as pd
import torch
import torch.nn as nn  # This is a modulo of torch to implement neural networks
import torch.nn.parallel # This is for paralle computation 
import torch.optim as optim # This is for optimizer
import torch.utils.data # This is the tools we use
from torch.autograd import Variable # This is for stochastic gradient descent

# Importing the dataset
movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1') # sep is our seperator # header: we don't have a header here so we put None # engine: to make sure that dataset is imported correctly # encoding: we put this because of some special charecters in the movies that cannot treatedd properly with the classic encoding UTF 8  # first column is movie id
users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1') # first column is the user ID # second column is the gender # third column is the age # forth column is some codes that corresponce to the user's job # last column is a code
ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1') # first column corresponce to the users so the number 1 corresponce to the first user (all of the 1's are same user).  # sceond column corresponce to the movies (movie's ID)  # Third column corresponce to the ratings  # forth column is the time stamps that we don't absoulutly care. this correspoce when the user rated the movie.

# Preparing the training set and test set
training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t') # in the ml-100k you can see 5 training and test splits of the whole dataset composed 100'000 ratings. u1.base(training_set), u1.test (test_set) until u5.base, u5.test so each of these are seprate trining set and test set. what's the use of these? that's to perform k-fold validation manually. but we are not going to perform k-fold validation in this section so we only perform on one of them # we will convert this to the array because in here we get a data frame and we prefer arrays # seperator in here is a tab, the default is comma. in here instead of 'sep' we use 'delimiter' # In here, training_set is 80% of the original dataset composed of 100'000 so that would be 80 percent 20 percent split which is a optimal train split
training_set = np.array(training_set, dtype = 'int') # Converting data frame to array  # Second argument is the type of this new array, and since we only have user IDs, movie IDs and ratings that are integers then we convert it to an array of integers
test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\t')  # Note that ratings or movies are diffrent in training_set and test_set
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies                                    # The reason we doing so is because in the next part we want to convert the training set and test set into a matrix that one matrix for training set and one matrix for test set which we set its line going to Users, and columns are going to be the Movies and cells are going to be the Ratings. # in each of these matrices we want to include Users and Movies. if the user didn't rate we input 0 in there. each cell of indexes u,i which u indicates number users and i is the movie. each cell u,i get a rating of movie i by user u
nb_users = int(max(max(training_set[:,0]), max(test_set[:,0]))) # The reason we are getting the max from these 2 dataset is because it is possible that each of those are containing the highest user ID. in here it's training set but for other datsets it's possible that, it test_set. # Another reason why we use 'max' is because there are some repetition in the user IDs and with this method we will get the highest number which is equal to total number of users. # the reason why, we used 'int' is because we want to force that we have integers, otherwise we will get an error.
nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))  # First column was for users and second column for movies

# Converting the data into an array with users in lines and movies in columns    # The reason we want to convert this is because it is required for the input of restricted boltzmann machine. in general, we create an array with Observations (users) in lines, and Features (movies) in columns.
def convert(data):  # The reason for crating a function is because we want to apply this on training set and test set
    new_data = []  # For creating the array with users in lines, and movies in columns.in here we won't make a 2-dimension numpy array. we will create a list of list. list of list means that we are going to have several lists, one list for each line. since we have 943 users then we are going to have a 943 lists will be a 1682 elements which is the number of movies. # This list of list is our final output. in here we initialize it as a list
    for id_users in range(1, nb_users + 1): # + 1 is because in range upper bound is excluded    
        id_movies = data[:, 1][data[:, 0] == id_users]  # In here we put the movie ID of the specific user that we are in the loop on  # data is training set and test set # Syntax in here: we want the second column of data (training_set or test_set) such that (second bracket) first column (user ID) is equal to 1. # second bracket is not list, it's condition
        id_ratings = data[:, 2][data[:, 0] == id_users] # Rating ID is in third column # this gives us the rating starting from the first user.
        ratings = np.zeros(nb_movies) # Creating a list of rating of the users. be careful that this is not id_rating. because the one that we want to get is not only the rating of the movies the user rated but also zeros when the user didn't rate the movie. so now we want to create the list of 1682 elements and for each of the movies, we get the rating of movie, if it has been rated and 0 if not. so id_rating is the rating of the movie that user rated  # in here we make the zero list and then replace it with the rating 
        ratings[id_movies - 1] = id_ratings  # Replacing the ratings by zeros list. # The trick in here is that the index in python starts at 0 and index at movie ID starts at 1
        new_data.append(list(ratings)) # In her we want to make sure that ratings is list. because we really need a list of list 
    return new_data
training_set = convert(training_set)  # If you check out the old dataset of training_set then you will notice that in the second column for the first user the movie ID starts at 2 so for the new training_set we see 0 for the first one 
test_set = convert(test_set)

# Converting the data into Torch tensors                                    # Soon we will start building the architect of neural network, and we will build this architecture by PyTorch Tensors # Tensors: arrays that contain elements of a single data type. so tensors are a multi-deminstion matrix but instead of being in numpy array, it's in pytorch array. (numpy can be used but it's not so efficient).# for autoencoder, we could even use tenserflow but pytorch gets a better result  # In here traing set going to be one torch tensor and test set going to be another torch tensor. there is going to two seperate multi-dimension matrices based on pytorch
training_set = torch.FloatTensor(training_set)  # FloadTensor: it will create an object of this class. this object will be torch tensor itself. torch tensot is a multi-deminsion matrix with a single type. in here single type is going to be float  # This is not a numpy array but torch tensor
test_set = torch.FloatTensor(test_set)  # Warning: after running this the training set and test set on the Variable Explorer will disappear because Variable Explorer and Spyder does not recognize the torch tensors since py torch is so recent (couple of weeks). but don't worry, variables will be exist but it won't be shown on the Variable Explorer # Up until now, it was data preprocessing which is same in Boltzmann machine and autoencoder. from now on we code for the Boltmann machine which is, to predict if the user like the movies or not

# Converting the ratings into binary rating 1 (Liked) or 0 (Not liked)       # because we have a rating from 1 to 5 in our training set and test set # the reason we use 0 and 1 instead of yes or no is becasue the predicted rating in our output must have the same fromat as the existing ratings in the input otherwise things will be inconsistent for RBM 
training_set[training_set == 0] = -1        # In our training set, we are going to replace the all the zeros in our original training set with -1. because these corresponse to movies that users didn't rate. since now the ratings going to be 0 and 1. the original zeros must have another value. so in here -1 means there isn no rating for the specific user.
training_set[training_set == 1] = 0        # In here we consider the one star and two star, as 0 # OR operator doesn't work in pytorch
training_set[training_set == 2] = 0
training_set[training_set >= 3] = 1        # above three star is 1.

test_set[test_set == 0] = -1
test_set[test_set == 1] = 0
test_set[test_set == 2] = 0
test_set[test_set >= 3] = 1

# Creating the architecture of the Neural Network                           
class RBM():        # In here we are building the probabolistic graphical model  # We are going to make 3 functions: 1- init function: to initialize the RBM object that will be created afterward. the init function actually happens in every class. we always have to start with init function because this init funciton is to define parameters of the object that will be created, once the class has been made 2- sample h: that will sample the probabilties of the hidden nodes given the visible nodes  3- sample v : that will sample the probabilities of visible nodes given the hidden nodes # So 1 class with 3 functions
    def __init__(self, nv, nh):  # We are going to have 3 arguments. 1. self: corresponse to the object that will be created afterward and it should be here all the time  # nv is the number of visible nodes # nh is the number of hidden nodes 
        self.W = torch.randn(nh, nv) # inide init function, we initialise the parameters of our future object # we need to initiaze the parameters that will optimize during the training of RBM that is the weights and bias # capital W stands for weights # this torch that runs nh and nv, initializes a tensor of size nh, nv according to a normal distribution, and besides, this normal distribution has a mean of 0 and variance of 1. so that's initializes all the weights for probabilities p of the visible nodes according to the hidden nodes 
        self.a = torch.randn(1, nh)   # initialising th bias # Remmeber that there is some bias for the probability of hidden node given the visible node and some bias for the probabilities of visible nodes given the hidden nodes. # we start with the second type. # a is the parameter of object (self)  # in here we use random again to initialize the weights, according to the normal distribution of mean 0 and bias and variance 1.  # Since there is one bias for each hidden node and we have nh hidden node, then we need to create and vector of nh element, which those elements all initialized to some numbers that follow a normal distribution. but note that we need to create an additional dimension corresponsing to the batch. therefor out vector shouldn't have one dimension (like a single vector), it should have 2 dimension. first dimesion corresponce to the batch, and second dimensio to the bias. # why do we need to create this fake dimension batch? because the function that we are going to use then in pytorch cannot accept a single input vector of one dimension. so in here we cannot add nh dirrectly, but to add 1 (batch) before nh (bias)
        self.b = torch.randn(1, nv)   # This is bias for visible nodes
    def sample_h(self, x):      # second fucntion is about sampling the hidden nodes accoring to the probabilities p(h) given v which is sigmoid activation function # why sample h: because during the training we will approximate the log likelihood gradient through Gibbs sampling. and to apply Gibbs sampling we need to compute the probabilities of hidden nodes given visible nodes. after having this probability, we can sample the activation of hidden nodes. # sample_h because it will return some samples of the diffrent hidden nodes of RBM. suppose we 100 hidden nodes in our RBM then this function will sample these hidden nodes that is for each of these 1000 hidden nodes, it will activate them to a certain probability that will compute in the same function. for each of these nodes, the probability is p(h) given v that is the probabilty of h is equal to 1 given value of v. that is this probability that is equal to activation function. # sample_h takes two arguments: 1- self in order to get access to the variable above 2- X: visible neural v in the probabilities p(h) given v
        wx = torch.mm(x, self.W.t())  # Computing the prbability of h given v that is the probability of hidden neuron equals 1 given the value of visible neurons which that is input vector of observations with all the ratings # the probabilty of h given v is sigmoid activation function. the sigmoid activation function applied to Wx (product of vector of weights and vector of visible neurons plus the bias which is Wx+a.) a is bias of hidden node. b is bias of visible node.  # the reason why we use torch is because we make the product of two tensors. to make that product we should use a function called mm # mm: to make product of two tensors # Inside mm function we put two matrices.  # self.W because W is attached to the object  # t is transpose: because of correctness of mathematics we should take transpose of that matrix weights
        activation = wx + self.a.expand_as(wx)  # Computing what is inside the activation function that is sigmoid function that is wx+bias # we call wx+bias activation # remmeber that each input vector will not treated individually but inside batches that is the new dimesntion that we were created before. and even if the batch contains one input vector or one input of bias, while that input vector is still in the batch, in that case we call it mini-batch. # In here we want to make sure that the bias is applied to each line of mini batch and to make sure of that we use a function will add again a new dimension for this bias we are adding, and this function is called expand_as # inside paranthesis in expand_as we should say as what we want to expand the bias which we want to expand it as wx
        p_h_given_v = torch.sigmoid(activation) # Computing the activation function that will activate the hidden node but remmber that this activation function represents the probability (that hidden node wil be activated according to the value of visible node)  # p_h_given_v: for example imagine that a user only likes a dramatic movies. if there is a hidden node that detected the specific feature corresponsing to the drama genre then for this user that has a high rating (that is 1) for that type of movies, the probability of that node (that is specific to the drama feature) given the visible node of that user that likes drama, p_h_given_v will be so high. # p_h_given_v is sigmoid of activation which has been written before this.
        return p_h_given_v, torch.bernoulli(p_h_given_v) # In here we want to return p_h_given_v and also sample h that is sample of all hidden neurons according to the p_h_given_v # In here we're making bernoulli RBM because we're creating a binary outcome (yes or no). it will also return bernoulli samples of that distribution, of that probability of h given v # p_h_given_v is an vector of nh element. # Suppose p_h_given_v is a vector of 100 elements. then each of the 100 elements corresponce to each of the 100 hidden nodes. and each of these elements is the probability of that the hidden node is activated. For example let's take the i'th element of this vector. while i'th element of this vector is the probability that the i'th hidden node is activated but remmember that's given the values of visible nodes (that is given the rating of the user we are dealing with). so the idea right now is to use this probabilities to sample the activation of each of these hidden node that is for each of the 100 hidden nodes, depending on the probability that we have with this hidden node which is p_h_given_v, we will activate (yes or no) this hidden neuron.  # How are we going to activate it? suppose for a hidden neuron i, the probability corresponding to that hidden neuron in p_h_given_v is 0.7 or 70 percent. then we take random number between 0 and 1. if this random number is below 0.7 or 70 percent then we will activate the neuron and if the random number is larger than 0.7 then we will not activate the neuron. that's how bernoulli sampling works. so we do that for each of the 100 hidden node and then we get and vector of zeros and ones. zeos corrsponce to the hidden nodes that were not activated after the sampling and ones coresponce to the neurons that were activated by the sampling # inside the torch.bernoulli() we have to input the distribution of which we are making that sampling and that is p_h_given_v                                                                                                                                                                                                 
    def sample_v(self, y):  # Now we do the same thing for visible node which is the probabilities that each of the visible node is equals to 1
        wy = torch.mm(y, self.W)  # x corresponse to the visible node and y corresponse to the hidden node # in here we are eliminating the transpose: since we are making the product of the hidden nodes and torch tensor of weights that is W for the probability P(v) given h and we should not use transpose. 
        activation = wy + self.b.expand_as(wy) # In here we want bias b not a
        p_v_given_h = torch.sigmoid(activation)
        return p_v_given_h, torch.bernoulli(p_v_given_h)
    def train(self, v0, vk, ph0, phk): # In this function we use contrast divergence that will aproximate the likelihood gradient (check the 'must read_RBM paper' section 5 for pseducode which is last three lines in here) # remmber that RBM is enerdy based model. it means that we have a energy based model that we are trying to minimize (by optimizing the weights) # beside being an energy based model, it can be seen as a probabilistic graphical model which in here the goal is to maximize the log-likelihood of the training set # for minimizing energy, and mazimizing the log-likelihood, we need to compute the gradient. since computing the gradient is too heavy for computers then we will aproximate it that we use contrastive divergence algorithms and this comes with Gibbs sampling # in the psedo code in the paper in the last line c is our bias a  # first argument going to be our input vector (containing the ratings of all the movies by one user) that we calling v0 # second argument is the visible nodes obtained after k sampling which that means after k round trips (or iteration) first from the visible node to the hidden and then the way back from hidden nodes to the visible node. # third argument is the vector of probabilities at the first iteration the hidden nodes equal to 1 given the values of v0 # Forth argument is the probabilities of the hidden nodes after k sampling given the values of visible nodes vk
        self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk) # first is the product of the probabilities that hidden nodes are equal to 1 given the input vector 0 # the reason for using transpose is to make things mathematicly correct
        self.b += torch.sum((v0 - vk), 0)  # The reason for adding 0 is to keep the format of B as a tensor of 2 dimension.
        self.a += torch.sum((ph0 - phk), 0)
nv = len(training_set[0])   # Now we are creating our first RBM object and in our first object only init function will take action and the rest of thos three function will take action only dutring the training. so basiclly parameters of init function will be created (initialized). all the weights in the tensor of weights or W will be initialized then all of the bias of p_h_given_v and p_v_given_h will be initialized  # the reason why only init fuction will take action is because it's a default function # to create this function we need nv and nh. # nv is a fixed paramer that is the number of movies that we have. nv is visible node and at the start the visible nodes are the ratings of all the movies by specific user so we have one visible node for each movie (1682)
nh = 100 # nh is the hidden node correnspond to the number of features that are going to be detected by RBM. we can choose any number but the relavent number is around 100 (of course you can find a better number by practicing) since we have 1682 visible node.
batch_size = 100 # there is one more paramer that has not been higlighted yet and that's our batch size. # when we build our algorithm we won't update the weights after each observation, we update it after several observation that will go into a batch # if batch_size is 1 then we are doing the online learning which is go through all of the obervations (which goes to the network) 1 by 1 and then update the weight # to get a fast training then we can get a larger size. let' start with 100 but this is totally tunable like the previous one and you can improve it by practicing.
rbm = RBM(nv, nh)  # now we are ready to create our first RBM object since we have our two parameters of init function.


# Training the RBM
nb_epoch = 10  # in here, we start by choosing the number of epoches  # The reason for choosing 10 is because we have a few observation (only 943) and beside we have binary value (0 and 1) so convergance will be reached pretty fast
for epoch in range(1, nb_epoch + 1): # in here we go through the 10 epochs and in each epoch, all of our observation will go through the network and we will update the weights after each observations of each batch passed through the network. at the end we get the final visible nodes with the new ratings for the movies that originally were not rated. # nb_epoch + 1 is equal to 11 but the upper bound doesn't count so it goes through 1 to 10
    train_loss = 0 # remmber for any deep learning algorithm that we make, we need a loss function to measure the error which is error between the predicted ratings (that is either 0 or 1) and the real ratings (0 or 1) # there diffrent options for using loss function: 1- RNC: the root mean square error (diffrence between the predicted ratings and real ratings). we will use this in auto encoder 2- simple difference in absolute distance: which measures the absolute diffrence between the predicted rating and the real rating. (used in RBM) # we initialize the loss to 0 because before we start training, this loss is equal to 0 and it will increase when we find some error between the predicted rating and the real rating.
    s = 0. # we need a counter because we want to normalize the train_loss and for doing so we divide the train_loss by counter # we initialize this to 0 as well but we want it flaod so a dot has been addded # the counter will be increamented after each epoch 
    for id_user in range(0, nb_users - batch_size, batch_size): # here the real training will happen and it happens with three function of sample_h, sample_v, train. before when we made these functions, it was regarding to one user and we want to do it for all users (remmber that it's all users in a batch) # first thing we do is to get the batches of these users and for doing so we need a for loop. # Note that we want to get batches of users (go through network) and then update the weights and NOT users one by one (go through network) then update the weights. # since the batch_size is 100 then first batch is users from index 0 to 99 and second batch is users from index 100 to 199 and so on. the last batch that go into the network will be the batch size of the users from index 943-100=843 to 943 # the third argument is the steps because we don't want to go from 1 to 1 but we want to go from 1 to 100 and then 100 to 200 and so on. the default is always one. so the third argument is our batch_size
        vk = training_set[id_user : id_user + batch_size] # inside this loop, first we will get separetly the input and our target. # the input is the rating of all the movies by the specific user that we are dealing with right now. # The target going be the same as input at the start and since the input will go through the Gibbs Chain and will be updated (at each roundtrip) to get the new ratings in each visible nodes then the input going to be cahnged but the target will keep its same initial value # we will call the input vk because this is vk that is going to be the output Gibbs sampling (after each k steps of the eandom walk). at the start vk is the input batch of oservations (rating of the users in the batch, the ratings that already existed) # in here want the batch of id_user until its next 100 (batch_size)
        v0 = training_set[id_user : id_user + batch_size] # we don't want to touch the target that is batch of our original ratings that at the end we want to compare it to our predicted ratings. # we want to measure the error between the predicted ratings and the real ratings to get the train_loss. # we call this v0 which is our ratings of the movies that were already rated by the 100 user in that batch
        ph0,_ = rbm.sample_h(v0) # before we start we need a third thing that is the initial probabiliies # the initial probabiliies is ph0 that ph0 is the probabilities that the hidden node at the start is equal to 1 given the real ratings (ratings of the movies that already rated by the users in our batch) so in here we use sample_h because it returns p_h_give_v # since the sample_h also returns bernoulli sample then we use a python trick that is to use underline (_) after ph0 so we only get the first element this function returns # since sample_h is a method of our RBM class then we need to get our rbm object # inside paranthesise we need to input the visible node because we want to sample the hidden node
        for k in range(10): # this is a loop for the k steps of contrastive divergence that we're going to make the Gibbs chain that we're going to do the k steps of the random walk. # Gibbs sampling cosist of making this Gibbs chain that there are several round trip from the visible node to the hidden node and then from these hidden nodes to the visible nodes. in each round trip in Gibbs sampling, the visible nodes are dated and step after sterp we get close to our good predicted ratings. 
            _,hk = rbm.sample_h(vk) # At the begining, we start with our input batch of observations that is the input ratings that all are in v0 and also is the batch of 100 users. we get all the ratings from all users and for all movies # So at first step of Gibbs sampling, from this batch input vector of original ratings, we are going to sample the first hidden node and we gonna do it with bernoulli sampling following p_h_given_v zero distribution. that is exactly what happens in the sample_h function # first step is to call sample_h on the visible node and it will get the first sampling of first hidden node # Since we want only second element of sample_h return, then we add underline at the first  # hk is the hidden node optained at the k step of contrastive divergence # inside sample_h() parantesis: since we are doing the sampling of the first hidden node given the values of first visible nodes (that is our original ratings), the first input our sample_h function is going to be v0. but be careful that v0 is the target which we don't want to change. so instead we get the vk instead. vk so far is our input batch of observations and then vk will be updated. we will update vk right after we update hk with the other sampling function which is sample_v
            _,vk = rbm.sample_v(hk) # In here we update the vk that vk is no longer v0. now vk is going to be sampled visible nodes after first step of Gibbs sampling. we get the sample by calling the sample_v function on the first sample of out hidden nodes that is hk  # here we get the first update of the visible node that is the visible node after the sampling. that's the first step of our random walk. that is the first step of Gibbs sampling # since we have for loop then it will continue by itself. at the end of loop, we get the tenth sample of hidden nodes and tenth sample of visible node. # after the last step of random walks, we can approximate the gradients (see algoritm in page 28. last three lines.) # so far we did algoritm in page 28, line 5 and 6
            vk[v0<0] = v0[v0<0] # there is one more important step: in here we don't want to learn where there is no rating that is for the cell that has -1 # we get our target v0 here becasue this wasn't change and it keeps the original ratings # here we want that they keep their -1 ratings. for doing so we get the original -1 ratings from the target because it has not been changed. # by doing this we make sure that the training is not done on these ratings that were not existed. we want only training that happed
        phk,_ = rbm.sample_h(vk) # before we apply the train function to update the weights and bias, we need phk because it requires in train function (v0, vk, ph0, phk) # in here we want the second elemnt of return funcion in sample_h so we add underline # sample_h is going to be apply on the last sample of visible node and the end (tehth step) of for loop
        rbm.train(v0, vk, ph0, phk)  # in train function we don't want to return anything. it just updates the weights according to the line 8,9,10 of algorithm (in article). # The weights and bias going to be updated toward the direction of maximum likelihood and therefor all of our probabilities p(v) given the states of hidden node will be more relavent. we get the largest weights for the probabilities that are the most significant. eventually that will lead us to some predicted ratings that are going to be close to the real ratings. speaking of being close to the real ratings, that's exactly what we do in the next step
        train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0])) # In here we update the train_loss. # we add error to the train_loss that is the diffrence between the predicted ratings and real original ratings of our target (v0). so now we compare vk that is the last of the last visible node after the last batch of users that went through network, to the v0 that is the target that hasn't changed since the begining. # There two ways of measuring the error: 1- RMSE (autoecoder) 2- Simple distance in absoulute values (Boltzmann machine) # abs is absolute value of a number. abs of -2 is 2 # we get the absolute value of the diffrence between the target and our prediction # the reason for using [v=>0] is because we want the ratings that are existing
        s += 1. # Updating the counter because the counter normalized to train us. so in here we just need to incremented by one in float
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s)) # Prining what is going to happen in the training that is when we execute the code we would like to see what happens. we want to see the number of epoches, to see in which epoch are we during the training, and to see how loss is decresing in these epoches # the reason for str is because we have integers but we want them in string # + is for concatenating # train_loss/s: we want to normalize the train loss (or normalized train_loss) so we divide it by the counter or s # after this we do the same thing with the test loss and we are hoping to get the same loss as training which is around 0.25 # 0.25 means we predicting 3 right prediction out of 4           

# Testing the RBM
test_loss = 0 # Getting the test set result is quite similar to the training set result. the only diffrence is that there is going to be no training so we remove the for loop over the epochs which is first two lines 
s = 0.
for id_user in range(nb_users): # in here we don't need a batch size because it is a technique only for the training. as we know batch_size is parameter that we choose to get better performance on training set (and therefor test set) so in here we delte everything about batch_size # the reason for eliminating 0 is because it's the default value
    v = training_set[id_user:id_user+1] # in here we are replacing batch_size by 1 because we want to make prediction for each of the users one by one # replace vk by v # v is our input which we would do the predictions
    vt = test_set[id_user:id_user+1] # replace v0 which is our target by vt # note that only in here we change the training_set by test_set and v we don't. the reson is, in vt we want to compare the real ratings of the test set to our predictions and in v we want to use it as input that will be used to activate the hidden neurons of our RBM to predict the ratings of the movies in test set that were not rated yet
    ph0,_ = rbm.sample_h(v0)
    if len(vt[vt>=0]) > 0: # the reason for using if else is because we only want to consider the ratings that exist # we choose vt because that's the target that contains the original ratings of the test set # in here we eliminated the loop the reason is because we want to do it in 1 step and not in 10 step. so remmber in test set result we don't want to do a k (or 10 in here) step of random walk but we want to a 1 step of random walk which we call it blind walk. for example, imagine you are blindfolded and you plan to walk 100 step of straight line. and you trained before to staying on a straight line by random steps (this is not ecatly random walk because in random walk the probabilities are the same). there is a high chance at the end of 100 you gonna be a little outside of the line but if you this on a one step there is a higher chance that you are on that straight line # So in here we were trained to make 10 steps blindfolded so at the end of these 10 line we stay on the straight line, so for one step we would be much better at it so in here instead of 10 steps we will do it for 1 step (or one iteration) # In random walk the probabilities are the same, in blind walk the probabilities are not the same  
        _,h = rbm.sample_h(v) # we remove the k because we don't have loop for the k steps anymore
        _,v = rbm.sample_v(h)
        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0])) # the reason for using this inside if, is because we want to calculate this for the existing ratings
        s += 1.
print('test loss: '+str(test_loss/s)) # This is excellend because we got test_loss of 2.4 which is near to train_loss
